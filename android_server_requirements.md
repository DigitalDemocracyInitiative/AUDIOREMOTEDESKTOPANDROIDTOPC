# Android WebSocket Server Requirements for PC Client Integration

This document outlines the fundamental requirements for an Android WebSocket server designed to interact with the provided Python PC client script. The primary goal is to stream audio from the PC to a conceptual "Gemini Live" service on the Android device and stream Gemini Live's audio output back to the PC.

## 1. Server Type

*   **WebSocket Server:** The Android application must implement a WebSocket server.
*   **Binary Data Handling:** The server must be capable of receiving and sending raw binary data, as this will be the format for the audio chunks. It should not expect text-based WebSocket messages for the audio stream.

## 2. Core Responsibilities

The Android WebSocket server has several key responsibilities:

### a. Receiving Audio from PC Client

*   **Accept Connections:** Listen for incoming WebSocket connections from the PC client on the configured IP address and port.
*   **Receive Binary Audio Data:** Once a connection is established, the server must continuously receive binary audio data chunks sent by the PC client. These chunks represent the audio captured from the PC's microphone.

### b. Feeding Audio to "Gemini Live" (Conceptual & Speculative)

This is the most challenging and conceptual part, as a direct public API for streaming audio into a service like "Gemini Live" is not assumed to exist. The server would need to employ a strategy to make the received audio available to Gemini Live. Potential conceptual approaches include:

*   **Accessibility Services:**
    *   If Gemini Live utilizes Android's accessibility framework for audio input or can be made to listen to an audio source made available via accessibility features, this could be a path. This is highly dependent on Gemini Live's specific implementation.
*   **Virtual Microphone/Audio Routing:**
    *   **OS Level (Highly Speculative/Complex):** Attempting to create a virtual microphone device or reroute audio at the Android OS level so that Gemini Live (or the whole system) picks up the audio streamed from the PC. This typically requires low-level Android OS modifications or features, possibly root access, and is not generally feasible for standard app development.
    *   **App-Specific Integration:** If Gemini Live itself offered a mechanism (e.g., an Intent, API, or background service) to accept an audio stream from another app, this would be ideal, but is unknown.
*   **URI Schemes/Intents (Less Likely for Raw Streams):**
    *   While typically used for discrete actions or files, investigate if Gemini Live exposes any specific Android Intents or URI schemes that could be triggered to process audio data. This is unlikely to support a continuous raw audio stream.

**Acknowledgement:** The method for successfully feeding live audio into a third-party application like "Gemini Live" without a dedicated API is highly speculative and presents significant technical hurdles. The chosen method would be the core innovation of the Android server app.

### c. Capturing "Gemini Live's" Audio Output

The server must capture the audio output generated by Gemini Live in response to the input it received (or any other audio Gemini Live produces). Potential Android mechanisms include:

*   **`MediaProjection` API:**
    *   This API (Android 5.0 Lollipop and later) allows capturing the device's screen and/or system audio output.
    *   Requires explicit user permission each time the capture starts.
    *   The server could use `MediaProjection` to capture all audio played by the system (or a specific app if targeting improves in newer Android versions) while Gemini Live is active.
*   **Accessibility Services:**
    *   If Gemini Live's audio output is routed in a way that accessibility services can intercept or record it (less common for raw audio, more for UI events with sound).
*   **Internal Audio Recording (Limited & Problematic):**
    *   Some Android versions or OEM customizations might offer ways to record "internal" or "system" audio. These are often restricted, may require special permissions, root access, or might not work reliably across all devices.
    *   Privacy implications are significant here.
*   **Loopback Recording (with Virtual Audio Device - Highly Complex):**
    *   Conceptually similar to virtual microphone input, creating a loopback audio output that Gemini Live uses and the server records from. Extremely complex and likely requires OS-level capabilities.

**Acknowledgement:** Capturing audio output from another app also has significant challenges, primarily related to user permissions (`MediaProjection`), system limitations, and privacy concerns.

### d. Sending Captured Audio to PC Client

*   **Binary Audio Data Transmission:** The server must take the captured audio data (from Gemini Live's output) and send it back to the connected PC client via the active WebSocket connection.
*   **Continuous Streaming:** This should be done in chunks, similar to how it's received, to enable real-time (or near real-time) playback on the PC.

## 3. Connection Parameters

*   **Listening Port:** The WebSocket server must listen on port `8765` (TCP), as this is the default port configured in the PC client script. (Allowing user configuration of this port on both client and server would be a good enhancement).
*   **Network Discovery:** The server must be running on a device (the Android phone) connected to the same local area network (LAN) as the PC client. The PC client will connect to the server using the Android phone's IP address on this network.

## 4. Data Handling

*   **Raw Binary Audio:** All audio data transmitted between the PC client and the Android server (in both directions) is raw binary audio bytes.
*   **No Audio Format Interpretation (by Server for WebSocket):** The WebSocket server itself does not necessarily need to interpret the deep specifics of the audio format (e.g., sample rate (44100 Hz), channels (mono), format (16-bit PCM) as defined in the PC client). Its primary role is to act as a conduit for these bytes.
    *   The PC client is responsible for encoding its microphone audio into this format.
    *   The conceptual "Gemini Live" interaction layer on Android would need to handle or be configured for this format for input.
    *   The audio capture mechanism on Android for Gemini Live's output will determine the format of the captured audio. The server might need to perform resampling or reformatting if this captured format doesn't match what the PC client expects (though the PC client is also somewhat generic in its output playback). For simplicity, it's assumed the goal is to send back audio in a compatible format, ideally the same as the input.

## 5. Error Handling & Resilience (Basic Expectations)

*   **Connection Stability:** The server should be ableto handle multiple connection attempts from a client without crashing.
*   **Graceful Disconnections:** If a PC client disconnects, the server should handle this gracefully, cleaning up resources associated with that connection and be ready to accept new connections.
*   **Resource Management:** Properly manage resources like network sockets, audio buffers, and any resources used for interacting with "Gemini Live" or audio capture.
*   **Logging:** Implement basic logging on the Android server side (e.g., using Android's Logcat) to record server status, connection events, data transmission indicators, and any errors encountered. This is crucial for debugging.
*   **Permissions Handling:** If using APIs like `MediaProjection`, the server app must correctly handle the permission request flow with the user.

This outline provides a foundational set of requirements. The actual implementation, especially parts (2b) and (2c) involving "Gemini Live," will require significant research and development based on the capabilities and constraints of the Android OS and the (hypothetical) "Gemini Live" application.
